{"ast":null,"code":"import { __awaiter as e } from \"../../../../node_modules/.pnpm/@rollup_plugin-typescript@11.1.6_rollup@4.24.4_tslib@2.6.3_typescript@4.9.5/node_modules/tslib/tslib.es6.js\";\nimport { errorNotSupportedOnPlatform as t } from \"../public/constants.js\";\nimport { EffectFailureReason as r } from \"../public/videoEffects.js\";\nimport { sendMessageToParent as i } from \"./communication.js\";\nimport { registerHandler as o } from \"./handlers.js\";\nimport { getApiVersionTag as a } from \"./telemetry.js\";\nimport { inServerSideRenderingEnvironment as n, ssrSafeWindow as s } from \"./utils.js\";\nconst d = \"v2\";\nfunction h(t, r, i, o) {\n  var a, s;\n  return e(this, void 0, void 0, function* () {\n    const e = f();\n    !n() && (null === (s = null === (a = window.chrome) || void 0 === a ? void 0 : a.webview) || void 0 === s || s.registerTextureStream(t, e)), l(yield u(t, i, o), new m(i, r), e.writable);\n  });\n}\nfunction c(t, r, i, o) {\n  var a, s;\n  return e(this, void 0, void 0, function* () {\n    const e = f();\n    !n() && (null === (s = null === (a = window.chrome) || void 0 === a ? void 0 : a.webview) || void 0 === s || s.registerTextureStream(t, e)), l(yield u(t, i, o), new v(i, r), e.writable);\n  });\n}\nfunction u(r, i, o) {\n  return e(this, void 0, void 0, function* () {\n    if (n()) throw t;\n    const e = s().chrome;\n    try {\n      null == o || o.reportGettingTextureStream(r);\n      const t = (yield e.webview.getTextureStream(r)).getVideoTracks();\n      if (0 === t.length) throw new Error(`No video track in stream ${r}`);\n      return null == o || o.reportTextureStreamAcquired(), t[0];\n    } catch (e) {\n      throw i(`Failed to get video track from stream ${r}, error: ${e}`), new Error(`Internal error: can't get video track from stream ${r}`);\n    }\n  });\n}\nfunction f() {\n  if (n()) throw t;\n  const e = window.MediaStreamTrackGenerator;\n  if (!e) throw t;\n  return new e({\n    kind: \"video\"\n  });\n}\nfunction l(e, t, r) {\n  new (0, s().MediaStreamTrackProcessor)({\n    track: e\n  }).readable.pipeThrough(new TransformStream(t)).pipeTo(r);\n}\nclass m {\n  constructor(t, r) {\n    this.notifyError = t, this.videoFrameHandler = r, this.transform = (t, r) => e(this, void 0, void 0, function* () {\n      const e = t.timestamp;\n      if (null !== e) try {\n        const i = yield this.videoFrameHandler({\n            videoFrame: t\n          }),\n          o = new VideoFrame(i, {\n            timestamp: e\n          });\n        r.enqueue(o), t.close(), i.close();\n      } catch (e) {\n        t.close(), this.notifyError(e);\n      } else this.notifyError(\"timestamp of the original video frame is null\");\n    });\n  }\n}\nclass w {\n  constructor(e, t) {\n    if (this.headerBuffer = e, this.notifyError = t, this.ONE_TEXTURE_INPUT_ID = 1869900081, this.INVALID_HEADER_ERROR = \"Invalid video frame header\", this.UNSUPPORTED_LAYOUT_ERROR = \"Unsupported texture layout\", this.headerDataView = new Uint32Array(e), this.headerDataView.length < 8) throw this.notifyError(this.INVALID_HEADER_ERROR), new Error(this.INVALID_HEADER_ERROR);\n    if (this.headerDataView[0] !== this.ONE_TEXTURE_INPUT_ID) throw this.notifyError(this.UNSUPPORTED_LAYOUT_ERROR), new Error(this.UNSUPPORTED_LAYOUT_ERROR);\n  }\n  get oneTextureLayoutId() {\n    return this.headerDataView[0];\n  }\n  get version() {\n    return this.headerDataView[1];\n  }\n  get frameRowOffset() {\n    return this.headerDataView[2];\n  }\n  get frameFormat() {\n    return this.headerDataView[3];\n  }\n  get frameWidth() {\n    return this.headerDataView[4];\n  }\n  get frameHeight() {\n    return this.headerDataView[5];\n  }\n  get multiStreamHeaderRowOffset() {\n    return this.headerDataView[6];\n  }\n  get multiStreamCount() {\n    return this.headerDataView[7];\n  }\n}\nclass E {\n  constructor(e, t) {\n    this.metadataMap = new Map(), this.AUDIO_INFERENCE_RESULT_STREAM_ID = 828664161, this.ATTRIBUTE_ID_MAP_STREAM_ID = 1296320833;\n    const r = new Uint32Array(e);\n    for (let i = 0, o = 0; i < t; i++) {\n      const t = r[o++],\n        i = r[o++],\n        a = r[o++],\n        n = new Uint8Array(e, i, a);\n      this.metadataMap.set(t, n);\n    }\n  }\n  get audioInferenceResult() {\n    return this.metadataMap.get(this.AUDIO_INFERENCE_RESULT_STREAM_ID);\n  }\n  get attributes() {\n    const e = this.metadataMap.get(this.ATTRIBUTE_ID_MAP_STREAM_ID);\n    if (void 0 === e) return;\n    const t = new Map(),\n      r = new TextDecoder(\"utf-8\");\n    let i = 0;\n    const o = e[i] + (e[++i] << 8) + (e[++i] << 16) + (e[++i] << 24);\n    for (let a = 0; a < o && i < e.length - 1; a++) {\n      const o = e[++i] + (e[++i] << 8) + (e[++i] << 16) + (e[++i] << 24),\n        a = e.findIndex((e, t, r) => 0 == e && t > i),\n        n = r.decode(e.slice(++i, a)),\n        s = this.metadataMap.get(o);\n      void 0 !== s && t.set(n, s);\n      i = a + (4 - (a - i) % 4 - 1);\n    }\n    return t;\n  }\n}\nclass v {\n  constructor(r, i) {\n    this.notifyError = r, this.videoFrameHandler = i, this.shouldDiscardAudioInferenceResult = !1, this.transform = (t, r) => e(this, void 0, void 0, function* () {\n      const e = t.timestamp;\n      if (null !== e) try {\n        const {\n            videoFrame: i,\n            metadata: {\n              audioInferenceResult: o,\n              attributes: a\n            } = {}\n          } = yield this.extractVideoFrameAndMetadata(t),\n          n = yield this.videoFrameHandler({\n            videoFrame: i,\n            audioInferenceResult: o,\n            attributes: a\n          }),\n          s = new VideoFrame(n, {\n            timestamp: e\n          });\n        r.enqueue(s), i.close(), t.close(), n.close();\n      } catch (e) {\n        t.close(), this.notifyError(e);\n      } else this.notifyError(\"timestamp of the original video frame is null\");\n    }), this.extractVideoFrameAndMetadata = r => e(this, void 0, void 0, function* () {\n      if (n()) throw t;\n      if (\"NV12\" !== r.format) throw this.notifyError(\"Unsupported video frame pixel format\"), new Error(\"Unsupported video frame pixel format\");\n      const e = {\n          x: 0,\n          y: 0,\n          width: r.codedWidth,\n          height: 2\n        },\n        i = new ArrayBuffer(e.width * e.height * 3 / 2);\n      yield r.copyTo(i, {\n        rect: e\n      });\n      const o = new w(i, this.notifyError),\n        a = {\n          x: 0,\n          y: o.multiStreamHeaderRowOffset,\n          width: r.codedWidth,\n          height: r.codedHeight - o.multiStreamHeaderRowOffset\n        },\n        s = new ArrayBuffer(a.width * a.height * 3 / 2);\n      yield r.copyTo(s, {\n        rect: a\n      });\n      const d = new E(s, o.multiStreamCount);\n      return {\n        videoFrame: new VideoFrame(r, {\n          timestamp: r.timestamp,\n          visibleRect: {\n            x: 0,\n            y: o.frameRowOffset,\n            width: o.frameWidth,\n            height: o.frameHeight\n          }\n        }),\n        metadata: {\n          audioInferenceResult: this.shouldDiscardAudioInferenceResult ? void 0 : d.audioInferenceResult,\n          attributes: d.attributes\n        }\n      };\n    }), o(a(d, \"videoEffectsUtils.transformerWithMetadata.constructor\"), \"video.mediaStream.audioInferenceDiscardStatusChange\", ({\n      discardAudioInferenceResult: e\n    }) => {\n      this.shouldDiscardAudioInferenceResult = e;\n    });\n  }\n}\nfunction p(e, t) {\n  return (o, n) => {\n    null == t || t.reportApplyingVideoEffect(o || \"\", n), e(o, n).then(() => {\n      null == t || t.reportVideoEffectChanged(o || \"\", n), i(a(d, \"videoEffectsUtils.reportVideoEffectChanged\"), \"video.videoEffectReadiness\", [!0, o, void 0, n]);\n    }).catch(e => {\n      const t = e in r ? e : r.InitializationFailure;\n      i(a(d, \"videoEffectsUtils.effectFailure\"), \"video.videoEffectReadiness\", [!1, o, t, n]);\n    });\n  };\n}\nexport { p as createEffectParameterChangeCallback, h as processMediaStream, c as processMediaStreamWithMetadata };","map":null,"metadata":{},"sourceType":"module","externalDependencies":[]}